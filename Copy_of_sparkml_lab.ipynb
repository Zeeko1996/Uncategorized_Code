{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of sparkml_lab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoEssi/Uncategorized_Code/blob/master/Copy_of_sparkml_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sngGYe4E6kZA",
        "colab_type": "text"
      },
      "source": [
        "# 0. Dependencies & Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vos4UC6uWi_P",
        "colab_type": "text"
      },
      "source": [
        "## 0.1. Related dependencies and environment variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MLw08R56jgg",
        "colab_type": "code",
        "outputId": "67a5c71e-a595-4bfa-9c27-97ebf565217a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Install spark-related dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://mirrors.viethosting.com/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "# Set up required environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 60kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 52.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=f7c66a9814016bb50b8262a0b56bf81325d2757102820e109301cb7e1fbea4cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4udTPSeXNb2c",
        "colab_type": "text"
      },
      "source": [
        "## 0.2. Initialize Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tperr2jL6s5x",
        "colab_type": "code",
        "outputId": "2f7e1395-b97a-4ad4-a6b6-63f106bb0914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')\n",
        " \n",
        "import pyspark\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"sparkml_lab\") \\\n",
        ".master(\"local[*]\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "# Check\n",
        "spark.sparkContext.getConf().getAll()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.port', '33781'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.app.id', 'local-1574473761736'),\n",
              " ('spark.app.name', 'sparkml_lab'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.driver.host', '2513d2630e42'),\n",
              " ('spark.ui.showConsoleProgress', 'true')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btng7cqIW034",
        "colab_type": "code",
        "outputId": "fe7f5193-63b7-44b9-e437-793e472c037b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "spark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2513d2630e42:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>sparkml_lab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f78abf8fb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM1tjlaoNYsM",
        "colab_type": "text"
      },
      "source": [
        "# 1. Import Housing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR2xI-PYNYQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "    os.makedirs(housing_path, exist_ok=True)\n",
        "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()\n",
        "\n",
        "fetch_housing_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95WuQBY1i03o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "housing_pd = load_housing_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDFRmpEbVpcH",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data Discovery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn73Qh56Vbye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#housing = spark.read.csv(\"/content/datasets/housing/housing.csv\", header = True)\n",
        "housing = spark.createDataFrame(housing_pd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ8K_oqxVzPo",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Schema and dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWMBXQaTWCD5",
        "colab_type": "text"
      },
      "source": [
        "Printing schema of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD05H8CmVwmX",
        "colab_type": "code",
        "outputId": "06eab405-44c3-4fc8-d52e-6d26ee368773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "housing.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            " |-- ocean_proximity: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIR0ntqjWGPk",
        "colab_type": "text"
      },
      "source": [
        "Printing number of records in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSb-HBI7WJr2",
        "colab_type": "code",
        "outputId": "5e033564-fd0a-481c-c90b-4efd719b6fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "housing.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20640"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_aBVAAGWKBL",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Look at the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtIU7mhQYnOS",
        "colab_type": "text"
      },
      "source": [
        "Printing first five records of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7B03NWcYljP",
        "colab_type": "code",
        "outputId": "d1251691-06a8-40d1-ef15-5a955d821f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "housing.take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(longitude=-122.23, latitude=37.88, housing_median_age=41.0, total_rooms=880.0, total_bedrooms=129.0, population=322.0, households=126.0, median_income=8.3252, median_house_value=452600.0, ocean_proximity='NEAR BAY'),\n",
              " Row(longitude=-122.22, latitude=37.86, housing_median_age=21.0, total_rooms=7099.0, total_bedrooms=1106.0, population=2401.0, households=1138.0, median_income=8.3014, median_house_value=358500.0, ocean_proximity='NEAR BAY'),\n",
              " Row(longitude=-122.24, latitude=37.85, housing_median_age=52.0, total_rooms=1467.0, total_bedrooms=190.0, population=496.0, households=177.0, median_income=7.2574, median_house_value=352100.0, ocean_proximity='NEAR BAY'),\n",
              " Row(longitude=-122.25, latitude=37.85, housing_median_age=52.0, total_rooms=1274.0, total_bedrooms=235.0, population=558.0, households=219.0, median_income=5.6431, median_house_value=341300.0, ocean_proximity='NEAR BAY'),\n",
              " Row(longitude=-122.25, latitude=37.85, housing_median_age=52.0, total_rooms=1627.0, total_bedrooms=280.0, population=565.0, households=259.0, median_income=3.8462, median_house_value=342200.0, ocean_proximity='NEAR BAY')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcrpy83iZcpQ",
        "colab_type": "text"
      },
      "source": [
        "Printing the number of records with population more than 10000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7nXkBNbZdGJ",
        "colab_type": "code",
        "outputId": "9decbc85-1493-4703-c5eb-c21968e3f9a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "housing.where(housing.population > 10000).collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(longitude=-121.92, latitude=37.53, housing_median_age=7.0, total_rooms=28258.0, total_bedrooms=3864.0, population=12203.0, households=3701.0, median_income=8.4045, median_house_value=451100.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-117.78, latitude=34.03, housing_median_age=8.0, total_rooms=32054.0, total_bedrooms=5290.0, population=15507.0, households=5050.0, median_income=6.0191, median_house_value=253900.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-117.87, latitude=34.04, housing_median_age=7.0, total_rooms=27700.0, total_bedrooms=4179.0, population=15037.0, households=4072.0, median_income=6.6288, median_house_value=339700.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-117.88, latitude=33.96, housing_median_age=16.0, total_rooms=19059.0, total_bedrooms=3079.0, population=10988.0, households=3061.0, median_income=5.5469, median_house_value=265200.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-118.78, latitude=34.16, housing_median_age=9.0, total_rooms=30405.0, total_bedrooms=4093.0, population=12873.0, households=3931.0, median_income=8.0137, median_house_value=399200.0, ocean_proximity='NEAR OCEAN'),\n",
              " Row(longitude=-118.09, latitude=34.68, housing_median_age=4.0, total_rooms=23386.0, total_bedrooms=4171.0, population=10493.0, households=3671.0, median_income=4.0211, median_house_value=144000.0, ocean_proximity='INLAND'),\n",
              " Row(longitude=-118.1, latitude=34.57, housing_median_age=7.0, total_rooms=20377.0, total_bedrooms=4335.0, population=11973.0, households=3933.0, median_income=3.3086, median_house_value=138100.0, ocean_proximity='INLAND'),\n",
              " Row(longitude=-118.46, latitude=34.4, housing_median_age=12.0, total_rooms=25957.0, total_bedrooms=4798.0, population=10475.0, households=4490.0, median_income=4.542, median_house_value=195300.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-121.61, latitude=36.69, housing_median_age=19.0, total_rooms=9899.0, total_bedrooms=2617.0, population=11272.0, households=2528.0, median_income=2.0244, median_house_value=118500.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-121.68, latitude=36.72, housing_median_age=12.0, total_rooms=19234.0, total_bedrooms=4492.0, population=12153.0, households=4372.0, median_income=3.2652, median_house_value=152800.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-121.79, latitude=36.64, housing_median_age=11.0, total_rooms=32627.0, total_bedrooms=6445.0, population=28566.0, households=6082.0, median_income=2.3087, median_house_value=118800.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-117.74, latitude=33.89, housing_median_age=4.0, total_rooms=37937.0, total_bedrooms=5471.0, population=16122.0, households=5189.0, median_income=7.4947, median_house_value=366300.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-117.12, latitude=33.52, housing_median_age=4.0, total_rooms=30401.0, total_bedrooms=4957.0, population=13251.0, households=4339.0, median_income=4.5841, median_house_value=212300.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-121.53, latitude=38.48, housing_median_age=5.0, total_rooms=27870.0, total_bedrooms=5027.0, population=11935.0, households=4855.0, median_income=4.8811, median_house_value=212200.0, ocean_proximity='INLAND'),\n",
              " Row(longitude=-121.4, latitude=38.47, housing_median_age=4.0, total_rooms=20982.0, total_bedrooms=3392.0, population=10329.0, households=3086.0, median_income=4.3658, median_house_value=130600.0, ocean_proximity='INLAND'),\n",
              " Row(longitude=-121.44, latitude=38.43, housing_median_age=3.0, total_rooms=39320.0, total_bedrooms=6210.0, population=16305.0, households=5358.0, median_income=4.9516, median_house_value=153700.0, ocean_proximity='INLAND'),\n",
              " Row(longitude=-117.75, latitude=34.01, housing_median_age=4.0, total_rooms=22128.0, total_bedrooms=3522.0, population=10450.0, households=3258.0, median_income=6.1287, median_house_value=289600.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-117.61, latitude=34.1, housing_median_age=9.0, total_rooms=18956.0, total_bedrooms=4095.0, population=10323.0, households=3832.0, median_income=3.6033, median_house_value=132600.0, ocean_proximity='INLAND'),\n",
              " Row(longitude=-116.14, latitude=34.45, housing_median_age=12.0, total_rooms=8796.0, total_bedrooms=1721.0, population=11139.0, households=1680.0, median_income=2.2612, median_house_value=137500.0, ocean_proximity='INLAND'),\n",
              " Row(longitude=-117.42, latitude=33.35, housing_median_age=14.0, total_rooms=25135.0, total_bedrooms=4819.0, population=35682.0, households=4769.0, median_income=2.5729, median_house_value=134400.0, ocean_proximity='<1H OCEAN'),\n",
              " Row(longitude=-117.27, latitude=33.15, housing_median_age=4.0, total_rooms=23915.0, total_bedrooms=4135.0, population=10877.0, households=3958.0, median_income=4.6357, median_house_value=244900.0, ocean_proximity='NEAR OCEAN'),\n",
              " Row(longitude=-120.59, latitude=34.7, housing_median_age=29.0, total_rooms=17738.0, total_bedrooms=3114.0, population=12427.0, households=2826.0, median_income=2.7377, median_house_value=28300.0, ocean_proximity='NEAR OCEAN'),\n",
              " Row(longitude=-118.9, latitude=34.26, housing_median_age=5.0, total_rooms=25187.0, total_bedrooms=3521.0, population=11956.0, households=3478.0, median_income=6.9712, median_house_value=321300.0, ocean_proximity='<1H OCEAN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVJ1Oq01dTE-",
        "colab_type": "text"
      },
      "source": [
        "## 2.3. Statistical summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys_fU2SNdagl",
        "colab_type": "text"
      },
      "source": [
        "Printing summary of the table statistics for the attributes housing_median_age, total_rooms, median_house_value, and population."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vmbDWgda3y",
        "colab_type": "code",
        "outputId": "8a10f42c-ecfa-4c71-de4a-1269e24b8ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "housing.describe([\"housing_median_age\", \"total_rooms\", \"median_house_value\", \"population\"]).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+\n",
            "|summary|housing_median_age|       total_rooms|median_house_value|        population|\n",
            "+-------+------------------+------------------+------------------+------------------+\n",
            "|  count|             20640|             20640|             20640|             20640|\n",
            "|   mean|28.639486434108527|2635.7630813953488|206855.81690891474|1425.4767441860465|\n",
            "| stddev|12.585557612111632|2181.6152515827957|115395.61587441375|1132.4621217653405|\n",
            "|    min|               1.0|               2.0|           14999.0|               3.0|\n",
            "|    max|              52.0|           39320.0|          500001.0|           35682.0|\n",
            "+-------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEc9vwVWdgpl",
        "colab_type": "text"
      },
      "source": [
        "Print the maximum age (housing_median_age), the minimum number of rooms (total_rooms), and the average of house values (median_house_value)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L73GSJtPdjwn",
        "colab_type": "code",
        "outputId": "09c0c5c7-bc97-4e1a-bf16-ce163f77e07f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "value1 = housing.agg({\"housing_median_age\": \"max\"}).collect()[0]\n",
        "value2 = housing.agg({\"total_rooms\" : \"min\"}).collect()[0]\n",
        "value3 = housing.agg({\"median_house_value\" : \"mean\"}).collect()[0]\n",
        "\n",
        "print(\"Maximum housing age: \" + str(value1[\"max(housing_median_age)\"]) + \n",
        "      \"\\nMinimum number of rooms: \" + str(value2[\"min(total_rooms)\"]) +\n",
        "      \"\\nAverage house value: \" + str(value3[\"avg(median_house_value)\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum housing age: 52.0\n",
            "Minimum number of rooms: 2.0\n",
            "Average house value: 206855.81690891474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vuj2BPqdnD0",
        "colab_type": "text"
      },
      "source": [
        "## 2.4. Data breakdown by categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5zlX535drcL",
        "colab_type": "text"
      },
      "source": [
        "Printing the number of houses in different areas (ocean_proximity), and sorted in descending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU5TB2rBdrwR",
        "colab_type": "code",
        "outputId": "ee7527b1-02a6-4e0e-8e83-192558109d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "housing.cube(\"ocean_proximity\").count().sort('count', ascending=False).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+-----+\n",
            "|ocean_proximity|count|\n",
            "+---------------+-----+\n",
            "|           null|20640|\n",
            "|      <1H OCEAN| 9136|\n",
            "|         INLAND| 6551|\n",
            "|     NEAR OCEAN| 2658|\n",
            "|       NEAR BAY| 2290|\n",
            "|         ISLAND|    5|\n",
            "+---------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY3hDSvjdr_l",
        "colab_type": "text"
      },
      "source": [
        "Printing the average value of the houses (median_house_value) in different areas (ocean_proximity); calling the new column avg_value when printing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEMBypJKyR-J",
        "colab_type": "code",
        "outputId": "f7de6a48-fa49-434e-ae29-7a3516b7dd5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "housing.\\\n",
        "select([\"ocean_proximity\", \"median_house_value\"]).\\\n",
        "groupBy(\"ocean_proximity\").\\\n",
        "agg(F.mean(\"median_house_value\").alias(\"avg_value\")).\\\n",
        "show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+------------------+\n",
            "|ocean_proximity|         avg_value|\n",
            "+---------------+------------------+\n",
            "|         ISLAND|          380440.0|\n",
            "|     NEAR OCEAN|249433.97742663656|\n",
            "|       NEAR BAY|259212.31179039303|\n",
            "|      <1H OCEAN|240084.28546409807|\n",
            "|         INLAND|124805.39200122119|\n",
            "+---------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA03bnsFd63G",
        "colab_type": "text"
      },
      "source": [
        "Rewritten in SQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYUI6IEmd7Pw",
        "colab_type": "code",
        "outputId": "479c9b95-e473-4893-ff6c-dbe2dd98cc59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "housing.createOrReplaceTempView(\"housing\")\n",
        "spark.sql('''\n",
        "          SELECT ocean_proximity, avg(median_house_value)\n",
        "          AS avg_value\n",
        "          FROM housing\n",
        "          GROUP BY ocean_proximity\n",
        "          ''').show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+------------------+\n",
            "|ocean_proximity|         avg_value|\n",
            "+---------------+------------------+\n",
            "|         ISLAND|          380440.0|\n",
            "|     NEAR OCEAN|249433.97742663656|\n",
            "|       NEAR BAY|259212.31179039303|\n",
            "|      <1H OCEAN|240084.28546409807|\n",
            "|         INLAND|124805.39200122119|\n",
            "+---------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTFtKZngds_8",
        "colab_type": "text"
      },
      "source": [
        "## 2.5. Correlation among attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7agCNR2dxoX",
        "colab_type": "text"
      },
      "source": [
        "Printing the correlation among the attributes housing_median_age, total_rooms, median_house_value, and population. To do so, first we need to put these attributes into one vector. Then, compute the standard correlation coefficient (Pearson) between every pair of attributes in this new vector. To make a vector of these attributes, we use the VectorAssembler Transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVNaKNeRdw2D",
        "colab_type": "code",
        "outputId": "afd50902-ed1f-4bfc-db37-df04b001becd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"housing_median_age\",\n",
        "                                       \"total_rooms\",\n",
        "                                       \"median_house_value\",\n",
        "                                       \"population\"],\n",
        "                             outputCol=\"attributes\")\n",
        "\n",
        "housing_attributes = assembler.transform(housing)\n",
        "\n",
        "housing_attributes.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|          attributes|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
            "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|[41.0,880.0,45260...|\n",
            "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|[21.0,7099.0,3585...|\n",
            "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|[52.0,1467.0,3521...|\n",
            "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|[52.0,1274.0,3413...|\n",
            "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|[52.0,1627.0,3422...|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR57yX60dyHo",
        "colab_type": "code",
        "outputId": "44f9a24b-110a-476e-95d5-1bd4ec77000a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "corr = Correlation.corr(housing_attributes, \"attributes\")\n",
        "\n",
        "corr.collect()[0][\"pearson({})\".format(\"attributes\")].values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.        , -0.3612622 ,  0.10562341, -0.29624424, -0.3612622 ,\n",
              "        1.        ,  0.13415311,  0.85712597,  0.10562341,  0.13415311,\n",
              "        1.        , -0.02464968, -0.29624424,  0.85712597, -0.02464968,\n",
              "        1.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFb6pehAdyyT",
        "colab_type": "text"
      },
      "source": [
        "## 2.6. Combine and make new attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt8Yy5UKd1Qa",
        "colab_type": "text"
      },
      "source": [
        "Now, let's try out various attribute combinations. In the given dataset, the total number of rooms in a block is not very useful, if we don't know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful, and we want to compare it to the number of rooms. And the population per household seems like also an interesting attribute combination to look at. To do so, we add the three new columns to the dataset as below. We will call the new dataset the housingExtra.\n",
        "\n",
        "```\n",
        "rooms_per_household = total_rooms / households\n",
        "bedrooms_per_room = total_bedrooms / total_rooms\n",
        "population_per_household = population / households\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkuJmTlyd1iJ",
        "colab_type": "code",
        "outputId": "60d25d49-c2ab-45a8-ff98-a665717064fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "new_attributes1 = housing.withColumn(\"rooms_per_household\",\n",
        "                                    housing.total_rooms / housing.households)\n",
        "\n",
        "new_attributes2 = new_attributes1.withColumn(\"bedrooms_per_room\",\n",
        "                                    new_attributes1.total_bedrooms / new_attributes1.total_rooms)\n",
        "\n",
        "new_housing = new_attributes2.withColumn(\"population_per_household\",\n",
        "                                    new_attributes2.population / new_attributes2.households)\n",
        "\n",
        "new_housing.select(\"rooms_per_household\",\n",
        "                   \"bedrooms_per_room\",\n",
        "                   \"population_per_household\").show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+------------------------+\n",
            "|rooms_per_household|  bedrooms_per_room|population_per_household|\n",
            "+-------------------+-------------------+------------------------+\n",
            "|  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n",
            "|  6.238137082601054|0.15579659106916466|       2.109841827768014|\n",
            "|  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n",
            "| 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n",
            "|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n",
            "|  4.761658031088083|0.23177366702937977|       2.139896373056995|\n",
            "| 4.9319066147859925|0.19289940828402366|      2.1284046692607004|\n",
            "|  4.797527047913447|0.22132731958762886|      1.7882534775888717|\n",
            "|  4.294117647058823| 0.2602739726027397|       2.026890756302521|\n",
            "|  4.970588235294118| 0.1992110453648915|       2.172268907563025|\n",
            "|  5.477611940298507|0.19709355131698456|       2.263681592039801|\n",
            "|  4.772479564032698| 0.2146731373108764|      2.0490463215258856|\n",
            "|  5.322649572649572| 0.1902850260939382|      2.3461538461538463|\n",
            "|                4.0| 0.2744252873563218|      1.9827586206896552|\n",
            "|  4.262903225806451|0.23685206205069997|      1.9548387096774194|\n",
            "|  4.242424242424242| 0.2526785714285714|       2.640151515151515|\n",
            "| 5.9395770392749245|0.17650050864699898|       2.395770392749245|\n",
            "|  4.052805280528053|0.23859934853420195|      2.1386138613861387|\n",
            "|  5.343675417661098|0.20321572130415363|      2.3627684964200477|\n",
            "|  5.465454545454546|0.19827012641383898|      2.5090909090909093|\n",
            "+-------------------+-------------------+------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tmZLVfnek2K",
        "colab_type": "text"
      },
      "source": [
        "# 3. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryFpQu8tZD9A",
        "colab_type": "text"
      },
      "source": [
        "Before going through the Machine Learning steps, let's first rename the label column from median_house_value to label.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-WYMOv0ZFpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "housing_renamed = new_housing.withColumnRenamed(\"median_house_value\", \"label\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drdgGCxUafFS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now, we want to separate the numerical attributes from the categorical attribute (ocean_proximity) and keep their column names in two different lists. Moreover, since we don't want to apply the same transformations to the predictors (features) and the label, we should remove the label attribute from the list of predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM6EFeqVqFvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns_features_num = housing_renamed.columns\n",
        "\n",
        "columns_features_num.remove(\"ocean_proximity\")\n",
        "columns_features_num.remove(\"label\")\n",
        "\n",
        "columns_features_cat = [\"ocean_proximity\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7DSxa71ruI2",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Prepare Continuous Attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8BQFI6bx2Tp",
        "colab_type": "text"
      },
      "source": [
        "### Data cleaning\n",
        "Most Machine Learning algorithms cannot work with missing features, so we should take care of them. As a first step, let's find the columns with missing values in the numerical attributes. To do so, we can print the number of missing values of each continues attributes, listed in colNum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rclN3Bwx0Q2k",
        "colab_type": "code",
        "outputId": "66bffc62-0fe8-4bff-e040-8686bfb52e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "null_df = housing_renamed.select([F.count(F.when(F.isnan(i) | \\\n",
        "                                          F.col(i).contains('NA') | \\\n",
        "                                          F.col(i).contains('NULL') | \\\n",
        "                                          F.col(i).isNull(), i)).alias(i) \\\n",
        "                                  for i in housing_renamed.columns])\n",
        "\n",
        "null_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-----+---------------+-------------------+-----------------+------------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|label|ocean_proximity|rooms_per_household|bedrooms_per_room|population_per_household|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-----+---------------+-------------------+-----------------+------------------------+\n",
            "|        0|       0|                 0|          0|           207|         0|         0|            0|    0|              0|                  0|              207|                       0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-----+---------------+-------------------+-----------------+------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlFl5Bja7JYm",
        "colab_type": "text"
      },
      "source": [
        "As we observerd above, the total_bedrooms and bedrooms_per_room attributes have some missing values. One way to take care of missing values is to use the Imputer Transformer, which completes missing values in a dataset, either using the mean or the median of the columns in which the missing values are located. To use it, you need to create an Imputer instance, specifying that you want to replace each attribute's missing values with the \"median\" of that attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Njcy_E7LhK",
        "colab_type": "code",
        "outputId": "1e087b8e-12a8-4f7d-b678-f4b6efec16b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(inputCols = [\"total_bedrooms\", \"bedrooms_per_room\"],\n",
        "                  outputCols = [\"total_bedrooms\", \"bedrooms_per_room\"])\n",
        "\n",
        "housing_imputed = imputer.fit(housing_renamed).transform(housing_renamed)\n",
        "\n",
        "housing_imputed.select([\"total_bedrooms\", \"bedrooms_per_room\"]).show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+-------------------+\n",
            "|total_bedrooms|  bedrooms_per_room|\n",
            "+--------------+-------------------+\n",
            "|         129.0|0.14659090909090908|\n",
            "|        1106.0|0.15579659106916466|\n",
            "|         190.0|0.12951601908657123|\n",
            "|         235.0|0.18445839874411302|\n",
            "|         280.0| 0.1720958819913952|\n",
            "+--------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbBgD_gV65Uo",
        "colab_type": "text"
      },
      "source": [
        "### Scaling\n",
        "One of the most important transformations we need to apply to our data is feature scaling. With few exceptions, Machine Learning algorithms don't perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the label attribues is generally not required.\n",
        "\n",
        "One way to get all attributes to have the same scale is to use standardization. In standardization, for each value, first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. To do this, we can use the StandardScaler Estimator. To use StandardScaler, again we need to convert all the numerical attributes into a big vectore of features using VectorAssembler, and then call StandardScaler on that vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1KS7wXVGLvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectas = VectorAssembler(inputCols = columns_features_num,\n",
        "                         outputCol = \"features\")\n",
        "\n",
        "housing_featured = vectas.transform(housing_imputed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbqq2pOaSrMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "scaler = StandardScaler(inputCol = \"features\",\n",
        "                        outputCol = \"scaled\",\n",
        "                        withStd = True)\n",
        "\n",
        "scalerModel = scaler.fit(housing_featured)\n",
        "\n",
        "housing_scaled = scalerModel.transform(housing_featured)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8FXnCsWDCLV",
        "colab_type": "code",
        "outputId": "d8b88bb9-40e6-4431-b6cd-bf51dd854ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "housing_scaled.printSchema()\n",
        "\n",
        "housing_scaled.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- label: double (nullable = true)\n",
            " |-- ocean_proximity: string (nullable = true)\n",
            " |-- rooms_per_household: double (nullable = true)\n",
            " |-- bedrooms_per_room: double (nullable = true)\n",
            " |-- population_per_household: double (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- scaled: vector (nullable = true)\n",
            "\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|            features|              scaled|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
            "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-122.23,37.88,41...|[-61.007269596069...|\n",
            "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|[-122.22,37.86,21...|[-61.002278409814...|\n",
            "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|[-122.24,37.85,52...|[-61.012260782324...|\n",
            "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|[-122.25,37.85,52...|[-61.017251968579...|\n",
            "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|[-122.25,37.85,52...|[-61.017251968579...|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCc2Hy0xFePN",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Prepare categorical attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1QumWjzFjEp",
        "colab_type": "text"
      },
      "source": [
        "After imputing and scaling the continuous attributes, we should take care of the categorical attributes. Let's first print the number of distict values of the categirical attribute ocean_proximity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbUGdBcQDcsm",
        "colab_type": "code",
        "outputId": "fa471d9c-35ea-4a38-a6f2-faa96e3d6abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "housing_renamed.agg(countDistinct(\"ocean_proximity\")).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------+\n",
            "|count(DISTINCT ocean_proximity)|\n",
            "+-------------------------------+\n",
            "|                              5|\n",
            "+-------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqJOIjc2GDXr",
        "colab_type": "text"
      },
      "source": [
        "### String indexer\n",
        "\n",
        "Most Machine Learning algorithms prefer to work with numbers. So let's convert the categorical attribute ocean_proximity to numbers. To do so, we can use the StringIndexer that encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M86Vd1MEGD3s",
        "colab_type": "code",
        "outputId": "c16b0412-8cd8-43e7-c82b-b447955f0f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol = \"ocean_proximity\", \n",
        "                        outputCol = \"ocean_proximity_indexed\")\n",
        "\n",
        "housing_indexed = indexer.fit(housing_renamed).transform(housing_renamed)\n",
        "\n",
        "housing_indexed.show(10)\n",
        "housing_indexed.agg(countDistinct(\"ocean_proximity_indexed\")).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|ocean_proximity_indexed|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n",
            "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|                    3.0|\n",
            "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|                    3.0|\n",
            "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|                    3.0|\n",
            "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|                    3.0|\n",
            "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|                    3.0|\n",
            "|  -122.25|   37.85|              52.0|      919.0|         213.0|     413.0|     193.0|       4.0368|269700.0|       NEAR BAY|  4.761658031088083|0.23177366702937977|       2.139896373056995|                    3.0|\n",
            "|  -122.25|   37.84|              52.0|     2535.0|         489.0|    1094.0|     514.0|       3.6591|299200.0|       NEAR BAY| 4.9319066147859925|0.19289940828402366|      2.1284046692607004|                    3.0|\n",
            "|  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|241400.0|       NEAR BAY|  4.797527047913447|0.22132731958762886|      1.7882534775888717|                    3.0|\n",
            "|  -122.26|   37.84|              42.0|     2555.0|         665.0|    1206.0|     595.0|       2.0804|226700.0|       NEAR BAY|  4.294117647058823| 0.2602739726027397|       2.026890756302521|                    3.0|\n",
            "|  -122.25|   37.84|              52.0|     3549.0|         707.0|    1551.0|     714.0|       3.6912|261100.0|       NEAR BAY|  4.970588235294118| 0.1992110453648915|       2.172268907563025|                    3.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+---------------------------------------+\n",
            "|count(DISTINCT ocean_proximity_indexed)|\n",
            "+---------------------------------------+\n",
            "|                                      5|\n",
            "+---------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW7y5kiwW5oB",
        "colab_type": "text"
      },
      "source": [
        "###One-hot encoding\n",
        "Now we convert the label indices built in the last step into one-hot vectors. To do this, we can take advantage of the OneHotEncoderEstimator Estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h5RUVrOXM1Z",
        "colab_type": "code",
        "outputId": "01bec613-a06c-4b72-d222-9c517c622908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "\n",
        "encoder = OneHotEncoderEstimator(inputCols = [\"ocean_proximity_indexed\"],\n",
        "                                 outputCols = [\"ocean_proximity_hot\"])\n",
        "\n",
        "housing_encoded = encoder.fit(housing_indexed).transform(housing_indexed)\n",
        "\n",
        "housing_encoded.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+-------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|ocean_proximity_indexed|ocean_proximity_hot|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+-------------------+\n",
            "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.25|   37.85|              52.0|      919.0|         213.0|     413.0|     193.0|       4.0368|269700.0|       NEAR BAY|  4.761658031088083|0.23177366702937977|       2.139896373056995|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.25|   37.84|              52.0|     2535.0|         489.0|    1094.0|     514.0|       3.6591|299200.0|       NEAR BAY| 4.9319066147859925|0.19289940828402366|      2.1284046692607004|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|241400.0|       NEAR BAY|  4.797527047913447|0.22132731958762886|      1.7882534775888717|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.26|   37.84|              42.0|     2555.0|         665.0|    1206.0|     595.0|       2.0804|226700.0|       NEAR BAY|  4.294117647058823| 0.2602739726027397|       2.026890756302521|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.25|   37.84|              52.0|     3549.0|         707.0|    1551.0|     714.0|       3.6912|261100.0|       NEAR BAY|  4.970588235294118| 0.1992110453648915|       2.172268907563025|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.26|   37.85|              52.0|     2202.0|         434.0|     910.0|     402.0|       3.2031|281500.0|       NEAR BAY|  5.477611940298507|0.19709355131698456|       2.263681592039801|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.26|   37.85|              52.0|     3503.0|         752.0|    1504.0|     734.0|       3.2705|241800.0|       NEAR BAY|  4.772479564032698| 0.2146731373108764|      2.0490463215258856|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.26|   37.85|              52.0|     2491.0|         474.0|    1098.0|     468.0|        3.075|213500.0|       NEAR BAY|  5.322649572649572| 0.1902850260939382|      2.3461538461538463|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.26|   37.84|              52.0|      696.0|         191.0|     345.0|     174.0|       2.6736|191300.0|       NEAR BAY|                4.0| 0.2744252873563218|      1.9827586206896552|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.26|   37.85|              52.0|     2643.0|         626.0|    1212.0|     620.0|       1.9167|159200.0|       NEAR BAY|  4.262903225806451|0.23685206205069997|      1.9548387096774194|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.26|   37.85|              50.0|     1120.0|         283.0|     697.0|     264.0|        2.125|140000.0|       NEAR BAY|  4.242424242424242| 0.2526785714285714|       2.640151515151515|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.27|   37.85|              52.0|     1966.0|         347.0|     793.0|     331.0|        2.775|152500.0|       NEAR BAY| 5.9395770392749245|0.17650050864699898|       2.395770392749245|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.27|   37.85|              52.0|     1228.0|         293.0|     648.0|     303.0|       2.1202|155500.0|       NEAR BAY|  4.052805280528053|0.23859934853420195|      2.1386138613861387|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.26|   37.84|              50.0|     2239.0|         455.0|     990.0|     419.0|       1.9911|158700.0|       NEAR BAY|  5.343675417661098|0.20321572130415363|      2.3627684964200477|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.27|   37.84|              52.0|     1503.0|         298.0|     690.0|     275.0|       2.6033|162900.0|       NEAR BAY|  5.465454545454546|0.19827012641383898|      2.5090909090909093|                    3.0|      (4,[3],[1.0])|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5BOO_oYJqj1",
        "colab_type": "text"
      },
      "source": [
        "# 4. Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9EzEUHkJs2D",
        "colab_type": "text"
      },
      "source": [
        "As you can see, there are many data transformation steps that need to be executed in the right order. For example, we called the Imputer, VectorAssembler, and StandardScaler from left to right. However, we can use the Pipeline class to define a sequence of Transformers/Estimators, and run them in order. A Pipeline is an Estimator, thus, after a Pipeline's fit() method runs, it produces a PipelineModel, which is a Transformer.\n",
        "\n",
        "Now, let's create a pipeline called numPipeline to call the numerical transformers you built above (imputer, va, and scaler) in the right order from left to right, as well as a pipeline called catPipeline to call the categorical transformers (indexer and encoder). Then, put these two pipelines numPipeline and catPipeline into one pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vnCWGl4JsYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "\n",
        "# Imputer\n",
        "imp = Imputer(inputCols = [\"total_bedrooms\", \"bedrooms_per_room\"],\n",
        "              outputCols = [\"total_bedrooms\", \"bedrooms_per_room\"])\n",
        "\n",
        "# VectorAssembler\n",
        "vec = VectorAssembler(inputCols = columns_features_num,\n",
        "                      outputCol = \"vfeatures\")\n",
        "\n",
        "# StandardScaler\n",
        "sca = StandardScaler(inputCol = \"vfeatures\",\n",
        "                     outputCol = \"scaled\",\n",
        "                     withStd = True)\n",
        "\n",
        "# numPipeline\n",
        "numPipeline = Pipeline(stages = [imp, vec, sca])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxEOx-inTNYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Indexer\n",
        "ind = StringIndexer(inputCol = \"ocean_proximity\", \n",
        "                    outputCol = \"ocean_proximity_indexed\")\n",
        "\n",
        "# Encoder\n",
        "enc = OneHotEncoderEstimator(inputCols = [\"ocean_proximity_indexed\"],\n",
        "                             outputCols = [\"ocean_proximity_hot\"])\n",
        "\n",
        "# catPipeline\n",
        "catPipeline = Pipeline(stages = [ind, enc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kax5R3p1Vkka",
        "colab_type": "code",
        "outputId": "86bf8aed-7a7c-4b4c-de20-793f92890cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Pipeline Assembly\n",
        "pipeline = Pipeline(stages = [numPipeline, catPipeline])\n",
        "\n",
        "# Fit and transform\n",
        "housing_new = pipeline.fit(housing_renamed).transform(housing_renamed)\n",
        "\n",
        "housing_new.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-----------------------+-------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|           vfeatures|              scaled|ocean_proximity_indexed|ocean_proximity_hot|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-----------------------+-------------------+\n",
            "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-122.23,37.88,41...|[-61.007269596069...|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|[-122.22,37.86,21...|[-61.002278409814...|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|[-122.24,37.85,52...|[-61.012260782324...|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|[-122.25,37.85,52...|[-61.017251968579...|                    3.0|      (4,[3],[1.0])|\n",
            "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|[-122.25,37.85,52...|[-61.017251968579...|                    3.0|      (4,[3],[1.0])|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-----------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5lCivZgmMrt",
        "colab_type": "text"
      },
      "source": [
        "Now, we use VectorAssembler to put all attributes of the final dataset housing_new into a big vector, and call the new column features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR9D6pq8mRfJ",
        "colab_type": "code",
        "outputId": "ea13170c-ef8f-4309-b060-e516d1776ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "newVec = VectorAssembler(inputCols = [\"scaled\", \"ocean_proximity_hot\"],\n",
        "                         outputCol = \"features\")\n",
        "\n",
        "dataset = newVec.transform(housing_new).select(\"features\", \"label\")\n",
        "\n",
        "dataset.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+\n",
            "|            features|   label|\n",
            "+--------------------+--------+\n",
            "|[-61.007269596069...|452600.0|\n",
            "|[-61.002278409814...|358500.0|\n",
            "|[-61.012260782324...|352100.0|\n",
            "|[-61.017251968579...|341300.0|\n",
            "|[-61.017251968579...|342200.0|\n",
            "+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADHLr1y2oMS4",
        "colab_type": "text"
      },
      "source": [
        "# 5. Make a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyR3wzttnAQW",
        "colab_type": "text"
      },
      "source": [
        "Here we going to make four different regression models:\n",
        "\n",
        "*   Linear regression model\n",
        "*   Decision tree regression\n",
        "*   Random forest regression\n",
        "*   Gradient-Boosted Forest Regression\n",
        "\n",
        "But, before giving the data to train a Machine Learning model, let's first split the data into training dataset (trainSet) with 80% of the whole data, and test dataset (testSet) with 20% of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrG0AadsnUCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set, test_set = dataset.randomSplit([0.8, 0.2], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXpEfeUyoPCr",
        "colab_type": "text"
      },
      "source": [
        "## 5.1. Linear regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUt5bRMeoTuX",
        "colab_type": "text"
      },
      "source": [
        "Now, we train a Linear Regression model using the LinearRegression class. Then, print the coefficients and intercept of the model, as well as the summary of the model over the training set by calling the summary method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHZr4UTLoREN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "lrModel = lr.fit(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LgZD6nxqNDp",
        "colab_type": "code",
        "outputId": "42a6fdac-ea51-4b1e-9c2b-54cef2d5c854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(lrModel.coefficients)\n",
        "print(lrModel.summary.rootMeanSquaredError)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-54477.3715845959,-55101.84003302382,13288.061745034573,10009.769723216179,710.726908834572,-52958.800287261125,46244.11745493835,77314.65490018335,5716.086889683943,15958.634084931,1399.7525958185972,-175517.1241143946,-211742.57639440673,-171874.40244509815,-180607.27387195334]\n",
            "67516.54425720422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFbLnEDfq2dt",
        "colab_type": "text"
      },
      "source": [
        "Now, we use RegressionEvaluator to measure the root-mean-square-erroe (RMSE) of the model on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LFX7whRrBTL",
        "colab_type": "code",
        "outputId": "c50b50e5-8377-4f90-f83e-5b6471949499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "predictions = lrModel.transform(test_set)\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
        "\n",
        "evaluator = RegressionEvaluator()\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(rmse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+--------+--------------------+\n",
            "|        prediction|   label|            features|\n",
            "+------------------+--------+--------------------+\n",
            "|182697.27426641854|111400.0|[-62.020480405854...|\n",
            "|146301.48449560022| 58100.0|[-61.995524474579...|\n",
            "|156240.51035895618| 72200.0|[-61.980550915813...|\n",
            "| 162194.0578350625| 70200.0|[-61.980550915813...|\n",
            "|199432.91476045595|128900.0|[-61.975559729558...|\n",
            "+------------------+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "69613.0558778766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJj56T4vrplq",
        "colab_type": "text"
      },
      "source": [
        "## 5.2. Decision tree regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prFC2kSiruNi",
        "colab_type": "text"
      },
      "source": [
        "Repeat what you have done on Regression Model to build a Decision Tree model. We use the DecisionTreeRegressor to make a model and then measure its RMSE on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkrBSVSjwGLX",
        "colab_type": "code",
        "outputId": "bfe1d3c1-3ffd-4c84-c63e-2f86758045eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "\n",
        "dtr = DecisionTreeRegressor()\n",
        "\n",
        "dtrModel = dtr.fit(training_set)\n",
        "\n",
        "predictions = dtrModel.transform(test_set)\n",
        "\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(rmse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68322.33909628939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ47-nnXwyt0",
        "colab_type": "text"
      },
      "source": [
        "## 5.3. Random forest regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ONcSvMRw1EO",
        "colab_type": "text"
      },
      "source": [
        "Let's try the test error on a Random Forest Model. We can use the RandomForestRegressor to make a Random Forest model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG7sTp0Rw8AD",
        "colab_type": "code",
        "outputId": "e5e9b086-654d-4eac-bdef-c09490d61769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "\n",
        "rfr = RandomForestRegressor()\n",
        "\n",
        "rfrModel = rfr.fit(training_set)\n",
        "\n",
        "predictions = rfrModel.transform(test_set)\n",
        "\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(rmse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65335.06730164033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdhjTYb9xQJO",
        "colab_type": "text"
      },
      "source": [
        "## 5.4. Gradient Boosted Tree Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N558NOTvxTbb",
        "colab_type": "text"
      },
      "source": [
        "Fianlly, we want to build a Gradient-boosted Tree Regression model and test the RMSE of the test data. We use the GBTRegressor to build the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRPxOm1ixVLK",
        "colab_type": "code",
        "outputId": "7fc5dc05-bc76-42ff-f942-2e0460c5dbef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "gbtr = GBTRegressor()\n",
        "\n",
        "gbtrModel = gbtr.fit(training_set)\n",
        "\n",
        "predictions = gbtrModel.transform(test_set)\n",
        "\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(rmse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56418.348519374646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWzTpMxxjkq",
        "colab_type": "text"
      },
      "source": [
        "# 6. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQv0LSIFxmAj",
        "colab_type": "text"
      },
      "source": [
        "An important task in Machie Learning is model selection, or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as LinearRegression, or for entire Pipelines which include multiple algorithms, featurization, and other steps. Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately. MLlib supports model selection tools, such as CrossValidator. These tools require the following items:\n",
        "\n",
        "*   Estimator\n",
        "*   ParamMaps\n",
        "*   Evaluator\n",
        "\n",
        "CrossValidator begins by splitting the dataset into a set of folds, which are used as separate training and test datasets. For example with k=3 folds, CrossValidator will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular ParamMap, CrossValidator computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs. After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset.\n",
        "\n",
        "Below, we use the CrossValidator to select the best Random Forest model. To do so, we need to define a grid of parameters. Let's say we want to do the search among the different number of trees (1, 5, and 10), and different tree depth (5, 10, and 15)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKFAESIwzFe-",
        "colab_type": "code",
        "outputId": "9506587d-aeeb-4827-ee97-e8020fa51e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "paramGrid = ParamGridBuilder().\\\n",
        "            addGrid(rfr.numTrees, [1, 5, 10]).\\\n",
        "            addGrid(rfr.maxDepth, [5, 10, 15]).build()\n",
        "\n",
        "cv = CrossValidator(estimator = rfr,\n",
        "                    estimatorParamMaps = paramGrid,\n",
        "                    evaluator = evaluator,\n",
        "                    numFolds = 3)\n",
        "\n",
        "cvModel = cv.fit(training_set)\n",
        "\n",
        "predictions = cvModel.transform(test_set)\n",
        "\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "print(rmse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53068.69884031164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3hr_Iud16z4",
        "colab_type": "text"
      },
      "source": [
        "# 7. Custom transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loAls6m3184v",
        "colab_type": "text"
      },
      "source": [
        "At the end of part two, we added extra columns to the housing dataset. Here, we are going to implement a Transformer to do the same task. The Transformer should take the name of two input columns inputCol1 and inputCol2, as well as the name of ouput column outputCol. It, then, computes inputCol1 divided by inputCol2, and adds its result as a new column to the dataset. The details of the implemeting a custom Tranfomer is explained [here](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types). Please read it before before we start to implement it.\n",
        "\n",
        "First, define the given parameters of the Transformer and implement a method to validate their schemas (StructType)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndpfx_h52KvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import StructField, StructType, DoubleType, StringType\n",
        "from pyspark.ml.param import Param, Params\n",
        "\n",
        "# Code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdph12_F2urn",
        "colab_type": "text"
      },
      "source": [
        "Then, we extend the class Transformer, and implement its setter functions for the input and output columns, and call then setInputCol1, setInputCol2, and setOutputCol. Morever, we need to override the methods copy, transformSchema, and the transform. The details of what we need to cover in these methods is given [here](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8Lz_QzT3k_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnveg6sV3qQt",
        "colab_type": "text"
      },
      "source": [
        "Now, an instance of MyTransformer, and set the input columns total_rooms and households, and the output column rooms_per_household and run it over the housing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z08WqJir3ubS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# myTransformer = MyTransformer(inputCol1 = \"total_rooms\",\n",
        "#                               inputCol2 = \"households\",\n",
        "#                               outputCol = \"rooms_per_household\")\n",
        "\n",
        "# myDataset = myTransformer.transform(housing).\n",
        "#             select(\"rooms_per_household\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PQh542H4HCx",
        "colab_type": "text"
      },
      "source": [
        "# 8. Custom estimator (predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6vYrOAq4JrN",
        "colab_type": "text"
      },
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzya2Q1J4NKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msngC6mP4U2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqxITBYT4UrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QRp_C5p4UfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSvINyl24T9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMgjytj-4Nyl",
        "colab_type": "text"
      },
      "source": [
        "# 9. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTW6yXeC4V58",
        "colab_type": "text"
      },
      "source": [
        "As the last step, we are given a dataset called data/ccdefault.csv. The dataset represents default of credit card clients. It has 30,000 cases and 24 different attributes. More details about the dataset is available at data/ccdefault.txt. In this task, we should make three models, compare their results and conclude the ideal solution. Here are the suggested steps:\n",
        "\n",
        "1.   Load the data.\n",
        "2.   Carry out some exploratory analysis\n",
        "3.   Train a model to predict the target variable\n",
        "4.   What else can we do with this data? Anything we can do to devise a better solution?\n",
        "\n",
        "In regards to training a model, we must:\n",
        "\n",
        "*   Employ three different models (logistic regression, decision tree, and random forest).\n",
        "*   Compare the models' performances (e.g. AUC).\n",
        "*   Defend your choice of best model (e.g., what are the strengths and weaknesses of each of these models?).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LzS-yd74Vbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}